---
---

@article{Szenes2024,
  bibtex_show={true},
	author = {Szenes, Kalman and Mörchen, Maximilian and Fischill, Paul and Reiher, Markus},
	title = {{Striking the right balance of encoding electron correlation in the Hamiltonian and the wavefunction ansatz}},
	journal = {Faraday Discussion on Correlated Electronic Structure},
	year = {2024},
	doi = {10.1039/D4FD00060A},
  selected = {true},
  abstract = {Multi-configurational electronic structure theory delivers the most versatile approximations to many-electron wavefunctions, flexible enough to deal with all sorts of transformations, ranging from electronic excitations, to open-shell molecules and chemical reactions. Multi-configurational models are therefore essential to establish universally applicable, predictive ab initio methods for chemistry. Here, we present a discussion of explicit correlation approaches which address the nagging problem of dealing with static and dynamic electron correlation in multi-configurational active-space approaches. We review the latest developments and then point to their key obstacles. Our discussion is supported by new data obtained with tensor network methods. We argue in favor of simple electron-only correlator expressions that may allow one to define transcorrelated models in which the correlator does not bear a dependence on molecular structure.}
}

@article{szenesDomainspecificImplementationHighorder2024,
  bibtex_show={true},
  title = {Domain-Specific Implementation of High-Order {{Discontinuous Galerkin}} Methods in Spherical Geometry},
  author = {Szenes, Kalman and Discacciati, Niccolò and Bonaventura, Luca and Sawyer, William},
  date = {2024-02-01},
  year={2024},
  journaltitle = {Computer Physics Communications},
  journal = {Computer Physics Communications},
  volume = {295},
  pages = {108993},
  issn = {0010-4655},
  doi = {10.1016/j.cpc.2023.108993},
  url = {https://www.sciencedirect.com/science/article/pii/S0010465523003387},
  urldate = {2023-11-12},
  abstract = {In recent years, domain-specific languages (DSLs) have achieved significant success in large-scale efforts to reimplement existing meteorological models in a performance portable manner. The dynamical cores of these models are based on finite difference and finite volume schemes, and existing DSLs are generally limited to supporting only these numerical methods. In the meantime, there have been numerous attempts to use high-order Discontinuous Galerkin (DG) methods for atmospheric dynamics, which are currently largely unsupported in main-stream DSLs. In order to link these developments, we present two domain-specific languages which extend the existing GridTools (GT) ecosystem to high-order DG discretization. The first is a C++-based DSL called G4GT, which, despite being no longer supported, gave us the impetus to implement extensions to the subsequent Python-based production DSL called GT4Py to support the operations needed for DG solvers. As a proof of concept, the shallow water equations in spherical geometry are implemented in both DSLs, thus providing a blueprint for the application of domain-specific languages to the development of global atmospheric models. We believe this is the first GPU-capable DSL implementation of DG in spherical geometry. The results demonstrate that a DSL designed for finite difference/volume methods can be successfully extended to implement a DG solver, while preserving the performance-portability of the DSL.},
  keywords = {Discontinuous Galerkin methods,Domain-specific languages,GPU programming},
  file = {/home/kszenes/Zotero/storage/FRC3TA3U/Szenes et al. - 2024 - Domain-specific implementation of high-order Disco.pdf;/home/kszenes/Zotero/storage/FURHDGAB/S0010465523003387.html},
  preview = {swe-3d.gif},
  selected = {true}
}



@article{bestaHighPerformanceProgrammableAttentional2023,
  bibtex_show={true},
  abbr={SC'23},
  title = {High-{{Performance}} and {{Programmable Attentional Graph Neural Networks}} with {{Global Tensor Formulations}}},
  author = {Besta, Maciej and Renc, Pawel and Gerstenberger, Robert and Sylos Labini, Paolo and Ziogas, Alexandros and Chen, Tiancheng and Gianinazzi, Lukas and Scheidl, Florian and Szenes, Kalman and Carigiet, Armon and Iff, Patrick and Kwasniewski, Grzegorz and Kanakagiri, Raghavendra and Ge, Chio and Jaeger, Sammy and Wąs, Jarosław and Vella, Flavio and Hoefler, Torsten},
  date = {2023-11-11},
  year={2023},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3581784.3607067},
  url = {https://doi.org/10.1145/3581784.3607067},
  urldate = {2023-11-04},
  journal = {Supercomputing},
  abstract = {Graph attention models (A-GNNs), a type of Graph Neural Networks (GNNs), have been shown to be more powerful than simpler convolutional GNNs (C-GNNs). However, A-GNNs are more complex to program and difficult to scale. To address this, we develop a novel mathematical formulation, based on tensors that group all the feature vectors, targeting both training and inference of A-GNNs. The formulation enables straightforward adoption of communication-minimizing routines, it fosters optimizations such as vectorization, and it enables seamless integration with established linear algebra DSLs or libraries such as GraphBLAS. Our implementation uses a data redistribution scheme explicitly developed for sparse-dense tensor operations used heavily in GNNs, and fusing optimizations that further minimize memory usage and communication cost. We ensure theoretical asymptotic reductions in communicated data compared to the established message-passing GNN paradigm. Finally, we provide excellent scalability and speedups of even 4--5x over modern libraries such as Deep Graph Library.},
  selected = {true}
}

